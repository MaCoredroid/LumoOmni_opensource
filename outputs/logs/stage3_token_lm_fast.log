`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:22<01:30, 22.58s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:40<00:59, 19.94s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:58<00:37, 18.81s/it]Loading checkpoint shards:  80%|████████  | 4/5 [01:15<00:18, 18.11s/it]Loading checkpoint shards: 100%|██████████| 5/5 [01:21<00:00, 14.04s/it]Loading checkpoint shards: 100%|██████████| 5/5 [01:21<00:00, 16.40s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
[train] epoch=0 step=0 loss=0.5244
[train] epoch=0 step=50 loss=0.6032
[train] epoch=0 step=100 loss=0.3642
[train] epoch=0 step=150 loss=0.2694
[train] epoch=0 step=200 loss=0.2555
[train] epoch=0 step=250 loss=0.2824
[train] epoch=0 step=300 loss=0.2028
[train] epoch=0 step=350 loss=0.1971
[train] epoch=0 step=400 loss=0.1510
[train] epoch=0 step=450 loss=0.2622
[train] epoch=0 step=500 loss=0.2204
[train] epoch=0 step=550 loss=0.2099
[train] epoch=0 step=600 loss=0.1222
[train] epoch=0 step=650 loss=0.1788
[train] epoch=0 step=700 loss=0.1833
[train] epoch=0 step=750 loss=0.1982
[train] epoch=0 step=800 loss=0.2143
[train] epoch=0 step=850 loss=0.1879
[train] epoch=0 step=900 loss=0.2166
[train] epoch=0 step=950 loss=0.2149
[train] epoch=0 step=1000 loss=0.2031
[train] epoch=0 step=1050 loss=0.2030
[train] epoch=0 step=1100 loss=0.1846
[train] epoch=0 step=1150 loss=0.2754
[train] epoch=0 step=1200 loss=0.1956
[train] epoch=0 step=1250 loss=0.1667
[train] epoch=0 step=1300 loss=0.1896
[train] epoch=0 step=1350 loss=0.1976
[train] epoch=0 step=1400 loss=0.3000
[train] epoch=0 step=1450 loss=0.1370
[train] epoch=0 step=1500 loss=0.1549
[train] epoch=0 step=1550 loss=0.2210
[train] epoch=0 step=1600 loss=0.1021
[train] epoch=0 step=1650 loss=0.2288
[train] epoch=0 step=1700 loss=0.2405
[train] epoch=0 step=1750 loss=0.1531
[train] epoch=0 step=1800 loss=0.2072
[train] epoch=0 step=1850 loss=0.2156
[train] epoch=0 step=1900 loss=0.1693
[train] epoch=0 step=1950 loss=0.1974
[train] epoch=0 step=2000 loss=0.2787
