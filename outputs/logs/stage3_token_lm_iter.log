`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:23<01:33, 23.49s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:43<01:03, 21.27s/it]Loading checkpoint shards:  60%|██████    | 3/5 [01:02<00:41, 20.58s/it]Loading checkpoint shards:  80%|████████  | 4/5 [01:20<00:19, 19.30s/it]Loading checkpoint shards: 100%|██████████| 5/5 [01:27<00:00, 14.97s/it]Loading checkpoint shards: 100%|██████████| 5/5 [01:27<00:00, 17.52s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
[train] epoch=0 step=0 loss=0.4965
[train] epoch=0 step=50 loss=0.4651
[train] epoch=0 step=100 loss=0.4356
[train] epoch=0 step=150 loss=0.7829
[train] epoch=0 step=200 loss=0.2782
[train] epoch=0 step=250 loss=0.2393
[train] epoch=0 step=300 loss=0.2738
[train] epoch=0 step=350 loss=0.3059
[train] epoch=0 step=400 loss=0.2718
[train] epoch=0 step=450 loss=0.3250
[train] epoch=0 step=500 loss=0.5246
[train] epoch=0 step=550 loss=0.1460
[train] epoch=0 step=600 loss=0.1279
[train] epoch=0 step=650 loss=0.1556
[train] epoch=0 step=700 loss=0.1241
[train] epoch=0 step=750 loss=0.3086
[train] epoch=0 step=800 loss=0.2285
[train] epoch=0 step=850 loss=0.1630
[train] epoch=0 step=900 loss=0.2420
[train] epoch=0 step=950 loss=0.2177
[train] epoch=0 step=1000 loss=0.1671
[train] epoch=0 step=1050 loss=0.2554
[train] epoch=0 step=1100 loss=0.1877
[train] epoch=0 step=1150 loss=0.2251
[train] epoch=0 step=1200 loss=0.2050
[train] epoch=0 step=1250 loss=0.1427
[train] epoch=0 step=1300 loss=0.3334
[train] epoch=0 step=1350 loss=0.1949
[train] epoch=0 step=1400 loss=0.2544
[train] epoch=0 step=1450 loss=0.3421
[train] epoch=0 step=1500 loss=0.1890
[train] epoch=0 step=1550 loss=0.2018
[train] epoch=0 step=1600 loss=0.3041
[train] epoch=0 step=1650 loss=0.2219
[train] epoch=0 step=1700 loss=0.2360
[train] epoch=0 step=1750 loss=0.1681
[train] epoch=0 step=1800 loss=0.1657
[train] epoch=0 step=1850 loss=0.1949
[train] epoch=0 step=1900 loss=0.4627
[train] epoch=0 step=1950 loss=0.1613
[train] epoch=0 step=2000 loss=0.2880
[train] epoch=0 step=2050 loss=0.1636
[train] epoch=0 step=2100 loss=0.1994
[train] epoch=0 step=2150 loss=0.1911
[train] epoch=0 step=2200 loss=0.1395
[train] epoch=0 step=2250 loss=0.3767
[train] epoch=0 step=2300 loss=0.1216
[train] epoch=0 step=2350 loss=0.1653
[train] epoch=0 step=2400 loss=0.1312
[train] epoch=0 step=2450 loss=0.2649
[train] epoch=0 step=2500 loss=0.4600
[train] epoch=0 step=2550 loss=0.2841
[train] epoch=0 step=2600 loss=0.3291
[train] epoch=0 step=2650 loss=0.5916
[train] epoch=0 step=2700 loss=0.2179
[train] epoch=0 step=2750 loss=0.1319
[train] epoch=0 step=2800 loss=0.1833
[train] epoch=0 step=2850 loss=0.1419
[train] epoch=0 step=2900 loss=0.1890
[train] epoch=0 step=2950 loss=0.1424
[train] epoch=0 step=3000 loss=0.1411
[train] epoch=0 step=3050 loss=0.2205
[train] epoch=0 step=3100 loss=0.1256
[train] epoch=0 step=3150 loss=0.2410
[train] epoch=0 step=3200 loss=0.1991
[train] epoch=0 step=3250 loss=0.2525
[train] epoch=0 step=3300 loss=0.1150
[train] epoch=0 step=3350 loss=0.1779
[train] epoch=0 step=3400 loss=0.3571
[train] epoch=0 step=3450 loss=0.1915
[train] epoch=0 step=3500 loss=0.1486
[train] epoch=0 step=3550 loss=0.1249
[train] epoch=0 step=3600 loss=0.1164
[train] epoch=0 step=3650 loss=0.2646
[train] epoch=0 step=3700 loss=0.1032
[train] epoch=0 step=3750 loss=0.1524
[train] epoch=0 step=3800 loss=0.1382
[train] epoch=0 step=3850 loss=0.1825
[train] epoch=0 step=3900 loss=0.0901
[train] epoch=0 step=3950 loss=0.2637
[train] epoch=0 step=4000 loss=0.1719
[train] epoch=0 step=4050 loss=0.1269
[train] epoch=0 step=4100 loss=0.2265
[train] epoch=0 step=4150 loss=0.1523
[train] epoch=0 step=4200 loss=0.2004
[train] epoch=0 step=4250 loss=0.2775
[train] epoch=0 step=4300 loss=0.1528
[train] epoch=0 step=4350 loss=0.3420
[train] epoch=0 step=4400 loss=0.1083
[train] epoch=0 step=4450 loss=0.2156
[train] epoch=0 step=4500 loss=0.1856
[train] epoch=0 step=4550 loss=0.2067
[train] epoch=0 step=4600 loss=0.2159
[train] epoch=0 step=4650 loss=0.1388
[train] epoch=0 step=4700 loss=0.1270
[train] epoch=0 step=4750 loss=0.1751
[train] epoch=0 step=4800 loss=0.1871
[train] epoch=0 step=4850 loss=0.2107
[train] epoch=0 step=4900 loss=0.1318
[train] epoch=0 step=4950 loss=0.1231
[train] epoch=0 step=5000 loss=0.1564
[train] epoch=0 step=5050 loss=0.0840
[train] epoch=0 step=5100 loss=0.2343
[train] epoch=0 step=5150 loss=0.2259
[train] epoch=0 step=5200 loss=0.3052
[train] epoch=0 step=5250 loss=0.0689
[train] epoch=0 step=5300 loss=0.1534
[train] epoch=0 step=5350 loss=0.2312
[train] epoch=0 step=5400 loss=0.1386
[train] epoch=0 step=5450 loss=0.1495
[train] epoch=0 step=5500 loss=0.1552
[train] epoch=0 step=5550 loss=0.1160
[train] epoch=0 step=5600 loss=0.1931
[train] epoch=0 step=5650 loss=0.1391
[train] epoch=0 step=5700 loss=0.1826
[train] epoch=0 step=5750 loss=0.2034
[train] epoch=0 step=5800 loss=0.1399
[train] epoch=0 step=5850 loss=0.1318
[train] epoch=0 step=5900 loss=0.1132
[train] epoch=0 step=5950 loss=0.1932
[train] epoch=0 step=6000 loss=0.2001
[train] epoch=0 step=6050 loss=0.1081
[train] epoch=0 step=6100 loss=0.0965
[train] epoch=0 step=6150 loss=0.2469
[train] epoch=0 step=6200 loss=0.1666
[train] epoch=0 step=6250 loss=0.0754
[train] epoch=0 step=6300 loss=0.1529
[train] epoch=0 step=6350 loss=0.2218
[train] epoch=0 step=6400 loss=0.1785
[train] epoch=0 step=6450 loss=0.1399
[train] epoch=0 step=6500 loss=0.2121
[train] epoch=0 step=6550 loss=0.1918
[train] epoch=0 step=6600 loss=0.1920
[train] epoch=0 step=6650 loss=0.1674
[train] epoch=0 step=6700 loss=0.1895
[train] epoch=0 step=6750 loss=0.0905
[train] epoch=0 step=6800 loss=0.2115
[train] epoch=0 step=6850 loss=0.1592
[train] epoch=0 step=6900 loss=0.1338
[train] epoch=0 step=6950 loss=0.1371
[train] epoch=0 step=7000 loss=0.1364
[train] epoch=0 step=7050 loss=0.2486
[train] epoch=0 step=7100 loss=0.2132
[train] epoch=0 step=7150 loss=0.1894
[train] epoch=0 step=7200 loss=0.1337
[train] epoch=0 step=7250 loss=0.1980
[train] epoch=0 step=7300 loss=0.1635
[train] epoch=0 step=7350 loss=0.1193
[train] epoch=0 step=7400 loss=0.1431
[train] epoch=0 step=7450 loss=0.1647
[train] epoch=0 step=7500 loss=0.1458
[train] epoch=0 step=7550 loss=0.1483
[train] epoch=0 step=7600 loss=0.1483
[train] epoch=0 step=7650 loss=0.1337
[train] epoch=0 step=7700 loss=0.2116
[train] epoch=0 step=7750 loss=0.1283
[train] epoch=0 step=7800 loss=0.1779
[train] epoch=0 step=7850 loss=0.1846
[train] epoch=0 step=7900 loss=0.1429
[train] epoch=0 step=7950 loss=0.1291
[train] epoch=0 step=8000 loss=0.1479
[train] epoch=0 step=8050 loss=0.2256
[train] epoch=0 step=8100 loss=0.1404
[train] epoch=0 step=8150 loss=0.2407
