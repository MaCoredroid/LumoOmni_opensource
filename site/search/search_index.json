{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"LumoOmni","text":"<p>LumoOmni is a staged research build toward an omni\u2011modal foundation model (vision + audio + text), starting from a strong text\u2011only LLM backbone and adding multimodal capability in increments that are auditable, reproducible, and regression\u2011gated.</p> <p>This documentation set is written like a small research website: it includes a mini paper, dataset notes, model/weight formats, training &amp; eval results, and a forward roadmap.</p>"},{"location":"#what-this-project-is-building","title":"What this project is building","text":""},{"location":"#track-a-continuousembedding-vlm-understanding-chat","title":"Track A \u2014 Continuous\u2011embedding VLM (understanding / chat)","text":"<p>A classic \u201cfrozen towers + trainable connector\u201d VLM stack:</p> <ul> <li>Text backbone: Qwen3\u20118B\u2011Base (frozen in Stage 1)</li> <li>Vision tower: SigLIP2 SO400M patch14\u2011384 (frozen in Stage 1)</li> <li>Connector: Perceiver\u2011style resampler + MLP projector (trainable)</li> </ul> <p>The connector produces a fixed number of visual tokens (e.g., 64) in the LLM hidden space and injects them at <code>&lt;image_patch&gt;</code> positions.</p>"},{"location":"#track-b-discretetoken-omni-model-generation-understanding","title":"Track B \u2014 Discrete\u2011token omni model (generation + understanding)","text":"<p>A \u201ctoken\u2011first\u201d path:</p> <ol> <li>Unified Token Interface (UTI) defines a stable ABI for converting text / image / audio \u2194 global token IDs.</li> <li>A token LM expands the LLM vocabulary to include those modality token IDs.</li> <li>Training begins by learning only the new vocabulary rows (\u201ctrainable\u2011rows\u2011only\u201d) before moving to multimodal pretraining.</li> </ol>"},{"location":"#current-milestones-high-level","title":"Current milestones (high level)","text":""},{"location":"#vlm-connector-sanity-and-scaling","title":"VLM connector sanity and scaling","text":"<ul> <li>Stage\u20111 ablation tests show the connector is being used (teacher\u2011forced loss: <code>correct &lt; shuffled &lt; zero/noise</code>) and truncation/label coverage are clean at <code>max_seq_len=512</code>.</li> </ul>"},{"location":"#uti-verified-with-regression-gates","title":"UTI verified with regression gates","text":"<ul> <li>Stage\u20113.1 UTI audit passes determinism, token\u2011range, decode sanity, and metric\u2011based regression gates (<code>metrics_v2_pass=true</code>).</li> </ul>"},{"location":"#weights-packaging-for-tokenlm-adapters","title":"Weights packaging for token\u2011LM adapters","text":"<ul> <li>Stage\u20113 deliverable packages the adapter as trainable\u2011rows\u2011only weights, plus <code>token_space.json</code>, with a simple patch\u2011load procedure.</li> </ul>"},{"location":"#where-to-start","title":"Where to start","text":"<ul> <li>Mini paper: see Mini paper in the navigation.</li> <li>If you want to reproduce: start with Reproducibility and Checkpoints &amp; weights.</li> <li>If you want to extend: start with Roadmap &amp; future work.</li> </ul>"},{"location":"#terminology-cheatsheet","title":"Terminology cheatsheet","text":"<ul> <li>Connector (VLM): resampler + projector that maps continuous vision embeddings into the LLM hidden space.</li> <li>UTI: unified token interface; stable token IDs and metadata for text/image/audio.</li> <li>Trainable rows: only the newly added embedding + LM\u2011head rows for non\u2011text tokens.</li> <li>A2T/I2T/T2A/T2I: audio/image \u2192 text and text \u2192 audio/image tasks used in token\u2011LM stages.</li> </ul>"},{"location":"checkpoints/","title":"Checkpoints &amp; weights","text":"<p>This page documents what artifacts exist, how they are saved, and how to load them.</p>"},{"location":"checkpoints/#track-a-vlm-connector-checkpoints","title":"Track A \u2014 VLM (connector checkpoints)","text":""},{"location":"checkpoints/#what-is-saved","title":"What is saved","text":"<p>In early stages, checkpoints typically include: - connector weights (resampler + projector) - training metadata (step, optimizer if desired) - pointers to base model versions</p> <p>The base LLM + vision tower are usually loaded from their original sources and kept frozen (Stage 1).</p>"},{"location":"checkpoints/#typical-paths-examples","title":"Typical paths (examples)","text":"<ul> <li><code>outputs/stage1_align_trial/checkpoint_3675.pt</code></li> <li><code>outputs/stage1_align_p11/...</code> (scaled Stage\u20111.1 runs)</li> </ul> <p>Adjust paths to match your repo layout. The important idea is \u201cconnector\u2011only\u201d or \u201cfrozen\u2011tower\u201d checkpoints until SFT.</p>"},{"location":"checkpoints/#track-b-token-lm-trainable-rows-adapters","title":"Track B \u2014 Token LM (\u201ctrainable rows\u201d adapters)","text":""},{"location":"checkpoints/#stage3-deliverable-format","title":"Stage\u20113 deliverable format","text":"<p>A Stage\u20113 adapter checkpoint is packaged as trainable\u2011rows\u2011only weights:</p> <p>Inside <code>checkpoint_7000/</code>: - <code>trainable_rows.pt</code> \u2014 modality rows for input embedding + LM head - <code>trainable_rows.json</code> \u2014 row ranges + base model id - <code>token_space.json</code> \u2014 global modality ranges + vocab sizes</p> <p>Base model weights are not saved; you load them and patch the new rows.</p>"},{"location":"checkpoints/#loading-example","title":"Loading example","text":"<pre><code>import torch\nfrom pathlib import Path\nfrom transformers import AutoModelForCausalLM\nfrom stage3_uti.tokenization.token_space import TokenSpace\nfrom stage3_uti.utils.train_utils import (\n    _load_trainable_rows,\n    _resize_and_init_embeddings,\n    resolve_pad_id,\n)\n\nbase_llm = \"Qwen/Qwen3-8B-Base\"\nckpt_dir = \"outputs/stage3_token_lm_iter5/checkpoint_7000\"\n\n# Load token space\nspace = TokenSpace.load_json(\"outputs/stage3_token_lm/token_space.json\")\ntext_vocab = int(space.text_vocab_size)\nvocab_total = int(space.vocab_size_total)\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_llm,\n    torch_dtype=torch.bfloat16,\n    low_cpu_mem_usage=True,\n)\n\n# Resize to include modality ranges\n_resize_and_init_embeddings(\n    model,\n    text_vocab_size=text_vocab,\n    vocab_size_total=vocab_total,\n    init_new_rows=True,\n)\nmodel.config.pad_token_id = resolve_pad_id(space)\n\n# Patch in the trained rows\n_load_trainable_rows(\n    model,\n    Path(ckpt_dir),\n    row_start=text_vocab,\n    row_end=vocab_total,\n)\n\nmodel.eval()\n</code></pre>"},{"location":"checkpoints/#why-this-format-exists","title":"Why this format exists","text":"<ul> <li>It keeps deliverables small.</li> <li>It enforces ABI stability via <code>token_space.json</code>.</li> <li>It isolates what was actually trained in Stage 3.</li> </ul>"},{"location":"checkpoints/#compatibility-rules-strongly-recommended","title":"Compatibility rules (strongly recommended)","text":"<ol> <li>Always ship <code>token_space.json</code> with any token\u2011LM checkpoint.</li> <li>Verify the token space hash matches the dataset you train/evaluate on.</li> <li>Record codec/tokenizer versions used to generate tokens.</li> </ol>"},{"location":"citation/","title":"Citation","text":"<p>If you publish work based on this project, add a citation entry like the following.</p> <p>Replace fields (authors, year, URL, version) with your actual metadata.</p> <pre><code>@misc{lumoOmni2026,\n  title        = {LumoOmni: Staged construction of an omni-modal model from a text-only LLM backbone},\n  author       = {Your Name and Collaborators},\n  year         = {2026},\n  howpublished = {Project documentation and code},\n  note         = {Version: v0.1 (living document)}\n}\n</code></pre>"},{"location":"citation/#what-to-cite-precisely","title":"What to cite precisely","text":"<ul> <li>The project version / git commit hash</li> <li>The <code>token_space.json</code> hash (for token\u2011LM results)</li> <li>The exact base model identifier for any adapter checkpoints</li> </ul>"},{"location":"datasets/","title":"Datasets","text":"<p>This page documents the datasets and data products used across stages. It is split into:</p> <ul> <li>Track A (VLM, continuous embeddings)</li> <li>Track B (UTI + token LM, discrete tokens)</li> </ul> <p>Note: dataset licenses vary. Treat this as documentation of usage and intended usage, not permission to redistribute.</p>"},{"location":"datasets/#track-a-vlm-continuous-embeddings","title":"Track A \u2014 VLM (continuous embeddings)","text":""},{"location":"datasets/#stage-1-alignment-captioningstyle-pretraining","title":"Stage 1: Alignment / captioning\u2011style pretraining","text":"<p>Primary dataset - LLaVA\u2011Pretrain (558k): image + caption/conversation JSON.</p> <p>Typical sample fields - <code>image</code>: relative image path - <code>conversations</code>: list of turns (human, gpt)</p> <p>Key notes - Prompt normalization ensures exactly one image placeholder; <code>&lt;image&gt;</code> is appended if missing. - Early runs used <code>max_seq_len=512</code> with no truncation pressure in the trial config.</p>"},{"location":"datasets/#stage-2-visual-instruction-tuning-single-image","title":"Stage 2: Visual instruction tuning (single image)","text":"<p>Primary dataset - LLaVA\u2011Instruct\u2011150K (single\u2011image filtered)</p> <p>Filtering rules (high level) - Exactly one resolvable image on disk. - Exactly one <code>&lt;image&gt;</code> placeholder in user content (or normalize into that format). - Skip missing/corrupt image files. - Skip empty assistant responses.</p>"},{"location":"datasets/#track-b-uti-token-lm-discrete-tokens","title":"Track B \u2014 UTI + token LM (discrete tokens)","text":""},{"location":"datasets/#stage-1-uti-token-space-codec-outputs","title":"Stage 1 (UTI): Token space + codec outputs","text":"<p>The goal is to define a stable internal token interface for: - text tokens (lossless), - image tokens + metadata required for decoding, - audio tokens + metadata required for decoding.</p> <p>A token space JSON + hash is treated as a first\u2011class artifact and must be consistent across: - tokenized datasets, - checkpoints, - evaluation scripts.</p>"},{"location":"datasets/#audio-datasets-recommended-pipeline-order","title":"Audio datasets (recommended pipeline order)","text":"<p>A practical progression for tokenization and early training:</p> <ol> <li>Clotho \u2014 small and clean: great for pipeline validation.</li> <li>AudioCaps \u2014 mid\u2011sized general audio captions.</li> <li>WavCaps \u2014 large; tokenize a subset first for iteration speed.</li> <li>Music caption data \u2014 include any clean music\u2011caption sources you have and oversample during training (often small).</li> </ol>"},{"location":"datasets/#image-dataset","title":"Image dataset","text":"<p>For early tokenization iterations, using the already\u2011wired image\u2011text pipeline is recommended:</p> <ul> <li>Start with LLaVA\u2011Pretrain (tokenize a manageable subset, e.g., 100k\u2013200k).</li> <li>Expand to larger/less\u2011curated sources after the pipeline is stable.</li> </ul>"},{"location":"datasets/#tokenized-data-products-stage-2-in-the-token-track","title":"Tokenized data products (Stage 2 in the token track)","text":""},{"location":"datasets/#output-types","title":"Output types","text":"<p>You typically want two separable outputs:</p> <ol> <li>Manifests</li> <li>canonical IDs</li> <li>file paths</li> <li>modality tags</li> <li>split assignment (train/eval)</li> <li>Tokenized shards</li> <li>token IDs (global space)</li> <li>per\u2011modality metadata needed for decode</li> <li>optional cached feature stats</li> </ol>"},{"location":"datasets/#verification-stage-2-verify","title":"Verification (Stage 2 Verify)","text":"<p>Before training on tokenized shards, run dataset\u2011scale verification analogous to the UTI audit: - token space hash matches - strict token range checks - decode spot checks - split integrity (no overlap) - retokenize consistency (when applicable) - dataloader smoke tests - baseline regression limits</p>"},{"location":"datasets/#common-pitfalls","title":"Common pitfalls","text":"<ul> <li>Path drift: manifests point to paths that moved after extraction (e.g., <code>.7z</code> archives not extracted).</li> <li>Codec config drift: sample rate / clip length changes silently and breaks retokenize assumptions.</li> <li>Split leakage: train/eval split generated from unstable IDs.</li> <li>Silent truncation: sequences exceed <code>max_seq_len</code> but labels are masked incorrectly.</li> </ul>"},{"location":"evaluation/","title":"Evaluation","text":"<p>LumoOmni prioritizes evaluation that answers one question early:</p> <p>\u201cIs the model actually using the modality signal we think it is using?\u201d</p> <p>That leads to ablation\u2011first evaluation (loss ordering) and audit\u2011first verification (token interfaces).</p>"},{"location":"evaluation/#track-a-continuousembedding-vlm","title":"Track A \u2014 Continuous\u2011embedding VLM","text":""},{"location":"evaluation/#ablation-test-teacherforced-loss","title":"Ablation test (teacher\u2011forced loss)","text":"<p>Evaluate loss on held\u2011out samples under: - correct image - shuffled image - zeroed visual tokens - random/noise visual tokens</p> <p>Pass criterion: <code>loss(correct) &lt; loss(shuffled) &lt; loss(zero/noise)</code>.</p> <p>This catches: - placeholder mismatch - injection bugs (tokens not inserted) - masking bugs (loss computed on wrong tokens) - \u201cvision ignored\u201d collapse</p>"},{"location":"evaluation/#truncation-label-coverage","title":"Truncation + label coverage","text":"<p>Track per epoch: - truncation rate (% samples truncated by <code>max_seq_len</code>) - % samples with <code>label_tokens == 0</code> - avg label tokens</p> <p>This catches: - prompt wrapper too long - formatting issues that mask labels</p>"},{"location":"evaluation/#stage-11-sweep-metrics","title":"Stage 1.1 sweep metrics","text":"<p>For sweep runs, compare: - eval loss (stratified by label length buckets) - ablation deltas (\u0394shuffle, \u0394zero) as a proxy for \u201cvision sensitivity\u201d - qualitative \u201cgolden set\u201d captions</p>"},{"location":"evaluation/#track-b-discrete-tokens-uti-token-lm","title":"Track B \u2014 Discrete tokens (UTI + token LM)","text":""},{"location":"evaluation/#uti-audit-promotion-gate","title":"UTI audit (promotion gate)","text":"<p>UTI audit checks: - determinism (text/image/audio) - token range correctness - shape sanity (token counts) - decode sanity (audio SR/length/channels; image sizes) - metric regression gates (PSNR/SSIM; log\u2011mel; SNR)</p> <p>Important: token\u2011id idempotence is not expected for lossy codecs and is treated as a non\u2011gate.</p>"},{"location":"evaluation/#token-lm-smoke-tests","title":"Token LM smoke tests","text":"<p>Minimal \u201cdoes it train\u201d tests: - overfit tiny set (loss drops) - resume from checkpoint - generation decode safety (short sequences decode without crash)</p>"},{"location":"evaluation/#token-lm-eval","title":"Token LM eval","text":"<p>Report loss by task: - <code>loss(all)</code> - <code>loss(a2t)</code> (audio \u2192 text) - <code>loss(i2t)</code> (image \u2192 text) - (later) <code>loss(t2a)</code>, <code>loss(t2i)</code> for generation stages</p>"},{"location":"evaluation/#what-is-not-claimed-yet","title":"What is not claimed yet","text":"<ul> <li>Strong conditional generation quality requires post\u2011run decode checks + prompt ablation, not just teacher\u2011forced loss.</li> <li>Final benchmarking (e.g., VQA, captioning, audio caption benchmarks) is out of scope for early stage gates.</li> </ul>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#is-this-a-single-model-or-multiple-models","title":"Is this a single model or multiple models?","text":"<p>Today it is a staged system with two tracks:</p> <ul> <li>a continuous\u2011embedding VLM stack (vision tower + connector + LLM)</li> <li>a discrete token track (UTI + token\u2011LM vocabulary expansion)</li> </ul> <p>Longer term, these can converge into one unified backbone.</p>"},{"location":"faq/#why-two-tracks","title":"Why two tracks?","text":"<p>They solve different problems:</p> <ul> <li>Continuous embeddings are a strong default for understanding and chat VLMs.</li> <li>Discrete tokens are a practical path for generation (emit image/audio tokens) once tokenization is stable.</li> </ul>"},{"location":"faq/#why-so-many-audits-and-gates","title":"Why so many audits and \u201cgates\u201d?","text":"<p>Because multimodal training failures are often silent. The goal is to catch: - injection bugs, - token\u2011space drift, - split leakage, - decode incompatibilities, before expensive training runs.</p>"},{"location":"faq/#what-does-trainablerowsonly-mean","title":"What does \u201ctrainable\u2011rows\u2011only\u201d mean?","text":"<p>Only the newly added vocabulary rows (input embedding + LM head) are trained and saved. You reconstruct the full model by loading the base LLM and patching those rows.</p>"},{"location":"faq/#are-psnrssim-numbers-low-in-uti-audit","title":"Are PSNR/SSIM numbers low in UTI audit?","text":"<p>They can be, depending on codec settings and decode path. Stage\u20111 UTI is about interface correctness and stability, not perceptual fidelity. Perceptual quality becomes a later\u2011stage concern.</p>"},{"location":"mini_paper/","title":"Mini paper (living document)","text":"<p>This is a \u201cmini paper\u201d intended for researchers who want a compact technical view of the project. It is a living document: numbers and stage status may evolve.</p>"},{"location":"mini_paper/#title","title":"Title","text":"<p>LumoOmni: Staged construction of an omni\u2011modal model from a text\u2011only LLM backbone</p>"},{"location":"mini_paper/#abstract","title":"Abstract","text":"<p>We describe a staged approach for building an omni\u2011modal model (vision + audio + text) starting from a high\u2011quality text\u2011only causal LLM. The approach has two parallel tracks. First, we build a continuous\u2011embedding VLM by freezing a vision tower and the LLM and training only a connector (Perceiver\u2011style resampler + projector) that injects a small number of visual tokens into the LLM sequence. Second, we establish a Unified Token Interface (UTI) that converts images and audio into discrete global token IDs with deterministic audits and regression gates; then we warm\u2011start the LLM\u2019s vocabulary by training only the new modality embedding/head rows (\u201ctrainable\u2011rows\u2011only\u201d). This decomposition keeps early iterations fast, isolates failure modes, and enables strict compatibility/versioning (token space hashes, retokenize checks, decode sanity).</p>"},{"location":"mini_paper/#1-motivation","title":"1. Motivation","text":"<p>Omni\u2011modal models are hard to debug because failures can be caused by: - data integrity (paths, serialization, truncation, split leakage), - tokenization interface drift (codec settings, resizers, sample rates), - placeholder or injection bugs (VLM token positions), - training instability from changing too many parameters at once.</p> <p>LumoOmni attempts to reduce these risks by: 1. freezing most of the model in early stages, 2. adding audits as first\u2011class gates, 3. packaging weights in patchable, versioned units.</p>"},{"location":"mini_paper/#2-method-overview","title":"2. Method overview","text":""},{"location":"mini_paper/#21-track-a-continuous-vlm-connector-alignment","title":"2.1 Track A: Continuous VLM connector alignment","text":"<p>We use a frozen vision encoder (SigLIP2) to produce patch embeddings and a trainable connector:</p> <ul> <li>Resampler: compresses <code>N_patches</code> embeddings into a fixed <code>N_latents</code> (e.g., 64) via cross\u2011attention.</li> <li>Projector: maps the resampler latents into the LLM hidden size.</li> </ul> <p>The projected tokens are inserted into the LLM input embeddings at <code>&lt;image_patch&gt;</code> positions.</p>"},{"location":"mini_paper/#22-track-b-discrete-token-omni-uti-vocab-warmstart","title":"2.2 Track B: Discrete token omni (UTI + vocab warm\u2011start)","text":"<p>We define a global token space that includes: - text token IDs (lossless), - image token IDs (codec\u2011based, lossy), - audio token IDs (codec\u2011based, lossy).</p> <p>UTI is treated like an ABI: the token space JSON + hash becomes part of every checkpoint and dataset artifact. Before training, we run an audit that checks determinism, token ranges, decode success, and regression metrics. Then we train the LLM to handle new modality tokens by learning only the new vocabulary rows (input embeddings + LM head), before later stages that add LoRA or full fine\u2011tuning.</p>"},{"location":"mini_paper/#3-experiments-and-results-selected","title":"3. Experiments and results (selected)","text":""},{"location":"mini_paper/#31-vlm-connector-sanity-via-ablation","title":"3.1 VLM connector sanity via ablation","text":"<p>Teacher\u2011forced loss is evaluated on held\u2011out samples under four visual conditions: - correct image, - shuffled image, - zeroed visual tokens, - random/noise visual tokens.</p> <p>A passing run shows <code>loss(correct) &lt; loss(shuffled) &lt; loss(zero/noise)</code>, providing evidence that visual embeddings are injected and attended to.</p>"},{"location":"mini_paper/#32-stage11-sweep-for-stable-hyperparameters","title":"3.2 Stage\u20111.1 sweep for stable hyperparameters","text":"<p>A small sweep over learning rate and optional vision LayerNorm is used to pick stable settings before scaling.</p>"},{"location":"mini_paper/#33-uti-audit-and-regression-gating","title":"3.3 UTI audit and regression gating","text":"<p>UTI verification reports determinism, range checks, decode sanity, and metric\u2011based regression gates (image PSNR/SSIM, audio log\u2011mel + SNR). This is treated as a promotion gate before large\u2011scale tokenized dataset generation.</p>"},{"location":"mini_paper/#34-trainablerows-adapter-deliverable","title":"3.4 Trainable\u2011rows adapter deliverable","text":"<p>A token\u2011LM checkpoint can be delivered as trainable\u2011rows\u2011only weights plus token space metadata, enabling lightweight distribution without bundling base model weights.</p>"},{"location":"mini_paper/#4-limitations","title":"4. Limitations","text":"<ul> <li>Image/audio tokenizations are lossy; strict token\u2011id idempotence is not expected.</li> <li>Current evaluations emphasize interface correctness and training signal; generative quality requires later\u2011stage decoding + conditional generation verification.</li> <li>Dataset licenses and redistribution constraints must be respected; tokenized derivatives may still inherit restrictions.</li> </ul>"},{"location":"mini_paper/#5-future-work","title":"5. Future work","text":"<ul> <li>Scale tokenized data generation and run larger MMPT (Stage 4+).</li> <li>Expand evaluation: conditional generation tests, human preference tests, and modality\u2011specific benchmarks.</li> <li>Explore joint training where the continuous VLM track and discrete token track share a backbone.</li> </ul>"},{"location":"mini_paper/#suggested-citation","title":"Suggested citation","text":"<p>See the Citation page for a BibTeX template.</p>"},{"location":"model_architecture/","title":"Model architecture","text":"<p>This project currently uses two complementary multimodal architectures.</p>"},{"location":"model_architecture/#track-a-continuousembedding-vlm","title":"Track A \u2014 Continuous\u2011embedding VLM","text":""},{"location":"model_architecture/#overview","title":"Overview","text":"<pre><code>image\n  \u2514\u2500 SigLIP2 vision tower (frozen) \u2192 patch embeddings (B, N_patches, 1152)\n        \u2514\u2500 Perceiver resampler (trainable) \u2192 latents (B, 64, 512)\n              \u2514\u2500 MLP projector (trainable) \u2192 tokens in LLM space (B, 64, 4096)\n                    \u2514\u2500 injected into Qwen3 input embedding sequence at &lt;image_patch&gt;\n                          \u2514\u2500 Qwen3 LLM (frozen in Stage 1; LoRA in Stage 2)\n</code></pre>"},{"location":"model_architecture/#concrete-connector-configuration","title":"Concrete connector configuration","text":"<ul> <li>Vision tower: SigLIP2 SO400M patch14\u2011384</li> <li>hidden size: 1152</li> <li>image size: 384</li> <li>patch size: 14</li> <li> <p><code>N_patches</code> = 27\u00d727 = 729</p> </li> <li> <p>Resampler (Perceiver\u2011style):</p> </li> <li><code>num_latents=64</code></li> <li><code>depth=2</code></li> <li><code>num_heads=8</code></li> <li><code>head_dim=64</code></li> <li> <p>latent dim = 512</p> </li> <li> <p>Projector (MLP):</p> </li> <li><code>512 \u2192 16384 \u2192 4096</code> (LLM hidden size 4096)</li> </ul>"},{"location":"model_architecture/#parameter-counts-connector-only","title":"Parameter counts (connector only)","text":"<ul> <li>Resampler: 6,927,872</li> <li>Projector: 75,517,952</li> <li>Total: 82,445,824 trainable parameters</li> <li>Optional <code>vision_ln</code>: 2,304 parameters (disabled in Stage 1)</li> </ul>"},{"location":"model_architecture/#code-locations-as-implemented","title":"Code locations (as implemented)","text":"<ul> <li>Resampler: <code>qwen3_vlm/models/resampler.py</code></li> <li>Projector: <code>qwen3_vlm/models/projector.py</code></li> <li>VLM wrapper / injection: <code>qwen3_vlm/models/vlm.py</code></li> <li>Training entry: <code>qwen3_vlm/train/train_utils.py</code></li> </ul>"},{"location":"model_architecture/#track-b-discrete-tokens-uti-token-lm","title":"Track B \u2014 Discrete tokens (UTI + token LM)","text":""},{"location":"model_architecture/#unified-token-interface-uti","title":"Unified Token Interface (UTI)","text":"<p>UTI defines: - how text is tokenized (lossless), - how images are encoded to discrete token IDs + decode metadata, - how audio is encoded to discrete token IDs + decode metadata, - a single global token space (ranges per modality) stored in <code>token_space.json</code>.</p> <p>The <code>token_space.json</code> + hash is treated as part of the model ABI.</p>"},{"location":"model_architecture/#token-lm-architecture","title":"Token LM architecture","text":"<p>The token LM is conceptually:</p> <ul> <li>Base: a text\u2011only causal LLM (Qwen3)</li> <li>Modification: resize vocab to include image/audio token ID ranges</li> <li>Training (Stage 3): learn only the new vocabulary rows</li> <li>input embeddings (added rows)</li> <li>LM head (added rows)</li> </ul> <p>Later stages can add: - LoRA on attention/MLP blocks, - full fine\u2011tuning, - modality\u2011aware sampling/decoding.</p>"},{"location":"model_architecture/#trainablerows-checkpointing","title":"Trainable\u2011rows checkpointing","text":"<p>To keep distribution lightweight: - Save only the trained rows (<code>trainable_rows.pt</code>) + metadata (<code>trainable_rows.json</code>) - Reconstruct full weights by loading the base model and patching in the trained rows - Always load the matching <code>token_space.json</code> (and ideally verify the hash)</p> <p>This is the discrete\u2011token analogue of \u201cconnector\u2011only checkpoints\u201d in the VLM track.</p>"},{"location":"reproducibility/","title":"Reproducibility","text":"<p>This project treats reproducibility as a first\u2011class feature:</p> <ul> <li>deterministic audits where possible</li> <li>stable token space hashes</li> <li>explicit artifacts/logs for each run</li> </ul> <p>Below are common \u201centry points\u201d for reproducing core checks.</p>"},{"location":"reproducibility/#track-a-vlm","title":"Track A \u2014 VLM","text":""},{"location":"reproducibility/#1-connector-ablation-evaluation","title":"1) Connector ablation evaluation","text":"<p>Run the ablation script against a trained checkpoint:</p> <ul> <li>correct image</li> <li>shuffled image</li> <li>zero/noise visual tokens</li> </ul> <p>Outputs: - per\u2011condition losses - deltas vs correct</p>"},{"location":"reproducibility/#2-truncation-label-coverage","title":"2) Truncation + label coverage","text":"<p>During training, log: - truncation rate - label_zero rate - avg label token counts</p>"},{"location":"reproducibility/#track-b-uti-token-lm","title":"Track B \u2014 UTI + token LM","text":""},{"location":"reproducibility/#1-uti-audit-deterministic","title":"1) UTI audit (deterministic)","text":"<p>Run the audit in deterministic mode and write: - <code>token_space.json</code> - <code>report.json</code> - reconstructed <code>recon.png</code> / <code>recon.wav</code></p> <p>Also run with: - baseline regression report for gating</p>"},{"location":"reproducibility/#2-token-lm-smoke-training","title":"2) Token LM smoke training","text":"<p>Run a short training job that confirms: - vocab resize works - checkpoints can be saved/resumed - short generation decodes without crash</p>"},{"location":"reproducibility/#artifacts-you-should-always-keep","title":"Artifacts you should always keep","text":"<ul> <li><code>token_space.json</code> + hash</li> <li>audit reports (<code>report.json</code>, <code>baseline_report.json</code>)</li> <li>train/eval split metadata</li> <li>full training logs (<code>train.log</code>)</li> <li>qualitative \u201cgolden sets\u201d (for VLM captioning or token decoding)</li> </ul>"},{"location":"reproducibility/#suggested-repo-structure-for-published-artifacts","title":"Suggested repo structure for published artifacts","text":"<pre><code>outputs/\n  stage1_.../\n    checkpoints/\n    metrics/\n    qual/\n  uti_audit/\n    token_space.json\n    token_space.sha256\n    report.json\n    baseline_report.json\n    recon.png\n    recon.wav\n  stage3_token_lm_.../\n    checkpoint_.../\n    eval_....json\n</code></pre>"},{"location":"roadmap/","title":"Roadmap &amp; future work","text":"<p>This roadmap is organized around incremental capability and auditability.</p>"},{"location":"roadmap/#near-term-next-milestones","title":"Near term (next milestones)","text":""},{"location":"roadmap/#1-scale-stage-2-tokenized-data-generation","title":"1) Scale Stage 2 tokenized data generation","text":"<ul> <li>Expand tokenized audio and image corpora beyond \u201citeration subsets\u201d.</li> <li>Implement dataset\u2011scale verify (split integrity, decode spot checks, retokenize consistency, regression baselines).</li> </ul>"},{"location":"roadmap/#2-run-a-larger-stage-3-warmstart","title":"2) Run a larger Stage 3 warm\u2011start","text":"<ul> <li>Train trainable\u2011rows on larger tokenized shards (more diversity, longer clips).</li> <li>Track text\u2011only regression invariance when patching rows.</li> </ul>"},{"location":"roadmap/#3-stage-4-multimodal-token-pretraining-mmpt","title":"3) Stage 4 multimodal token pretraining (MMPT)","text":"<p>Goal: learn conditional generation (T2I/T2A) while preserving I2T/A2T.</p> <p>Key verification requirements: - decode artifacts are within expected bounds (no shape errors, stable sample rates) - prompt ablations demonstrate conditional dependence (not unconditional memorization) - mixing ratios prevent \u201cread\u201d collapse</p>"},{"location":"roadmap/#mid-term","title":"Mid term","text":""},{"location":"roadmap/#expand-evaluation","title":"Expand evaluation","text":"<ul> <li>Conditional generation evaluation (prompt fidelity, diversity, failure rate).</li> <li>Modality\u2011specific benchmarks:</li> <li>image captioning/VQA (for Track A)</li> <li>audio captioning and retrieval (for Track B)</li> <li>Human preference tests for edit/generate tasks if deploying a tool\u2011augmented system.</li> </ul>"},{"location":"roadmap/#unify-tracks","title":"Unify tracks","text":"<p>A longer\u2011term direction is to unify continuous VLM understanding and discrete token generation into a single backbone, with a curriculum that stabilizes both.</p>"},{"location":"roadmap/#longer-term","title":"Longer term","text":""},{"location":"roadmap/#replace-frozen-tools-with-learned-generators","title":"Replace \u201cfrozen tools\u201d with learned generators","text":"<p>If using a tool\u2011augmented image generation/editing system: - keep tools frozen at first (fast iteration), - later train internal generator/editor models, - keep strict artifact logging for reproducibility (prompt, seed, mask, params, verifier scores).</p>"},{"location":"roadmap/#speech-output","title":"Speech output","text":"<p>The architecture and staging are designed to eventually support speech output (and potentially speech input) without rebuilding earlier pieces.</p>"},{"location":"roadmap/#open-questions","title":"Open questions","text":"<ul> <li>Best curriculum for mixing A2T/I2T with T2A/T2I.</li> <li>How to allocate context length between text and long audio token sequences.</li> <li>Which evaluation metrics best predict perceptual quality for decoded tokens (especially music).</li> <li>How to prevent text regression when expanding vocab and adding multimodal objectives.</li> </ul>"},{"location":"training/","title":"Training","text":"<p>This page summarizes how training is staged and why. The emphasis is on controlling degrees of freedom (freeze most things early), plus audits/regression gates.</p>"},{"location":"training/#track-a-continuousembedding-vlm","title":"Track A \u2014 Continuous\u2011embedding VLM","text":""},{"location":"training/#stage-1-connector-alignment-captioningstyle","title":"Stage 1: Connector alignment (captioning\u2011style)","text":"<p>Goal: Make the frozen LLM attend to injected visual tokens.</p> <p>Trainable - resampler + projector (connector) - optional vision LayerNorm (typically off initially)</p> <p>Frozen - Qwen3 LLM - SigLIP2 vision tower</p> <p>Typical config highlights - <code>num_image_tokens=64</code> - <code>max_seq_len=512</code> for early trials - bf16 precision - teacher\u2011forced next token prediction</p> <p>Key sanity gates - ablation loss ordering: correct &lt; shuffled &lt; zero/noise - truncation rate and label coverage</p>"},{"location":"training/#stage-11-stability-sweep-qualitative-harness","title":"Stage 1.1: Stability sweep + qualitative harness","text":"<p>Goal: Select stable hyperparameters and harden evaluation.</p> <p>Common sweep knobs: - learning rate - optional <code>vision_ln</code></p> <p>Added artifacts: - stratified eval set by target length - \u201cgolden set\u201d qualitative caption dumps at checkpoints</p>"},{"location":"training/#stage-2-visual-instruction-tuning-sft","title":"Stage 2: Visual instruction tuning (SFT)","text":"<p>Goal: Turn the aligned model into a chat VLM via instruction tuning.</p> <p>Typical approach: - Keep connector training - Add LoRA on the LLM (selected attention/MLP modules) - Train on single\u2011image instruction data (e.g., LLaVA\u2011Instruct\u2011150K)</p>"},{"location":"training/#track-b-discrete-tokens-uti-token-lm","title":"Track B \u2014 Discrete tokens (UTI + token LM)","text":""},{"location":"training/#stage-1-uti-implementation-verification","title":"Stage 1: UTI implementation + verification","text":"<p>Goal: Define a stable ABI for tokenizing and decoding text/image/audio.</p> <p>Core requirements: - deterministic tokenization paths (for audit mode) - strict token range checks - decode sanity (sizes, sample rates, channel counts) - regression gating on metrics</p>"},{"location":"training/#stage-2-tokenized-dataset-generation","title":"Stage 2: Tokenized dataset generation","text":"<p>Goal: Produce tokenized shards (train/eval) that are stable and verifiable.</p> <p>Key gates (dataset\u2011scale): - token space hash matches - split integrity (no overlap) - decode spot checks - retokenize consistency (when applicable) - dataloader smoke tests</p>"},{"location":"training/#stage-3-warmstart-multimodal-vocabulary-trainable-rows","title":"Stage 3: Warm\u2011start multimodal vocabulary (\u201ctrainable rows\u201d)","text":"<p>Goal: Teach the LLM to treat new modality token IDs as meaningful symbols while keeping text behavior stable.</p> <p>Trainable: - newly added rows in   - input embedding   - LM head</p> <p>Frozen: - all original weights (initially)</p> <p>Deliverable: - <code>trainable_rows.pt</code> + <code>trainable_rows.json</code> + <code>token_space.json</code></p>"},{"location":"training/#stage-4-multimodal-token-pretraining-mmpt","title":"Stage 4: Multimodal token pretraining (MMPT)","text":"<p>Goal: Teach the model to emit image/audio tokens: - T2I: text \u2192 image tokens - T2A: text \u2192 audio tokens</p> <p>while preserving: - I2T / A2T</p> <p>This is where LoRA/full\u2011tune and curriculum decisions start to matter.</p>"},{"location":"training/#design-choices-that-keep-iteration-fast","title":"Design choices that keep iteration fast","text":"<ul> <li>\u201cOne epoch \u2248 1 hour\u201d sizing is enforced by measuring step time on real sequences and back\u2011computing epoch sample counts.</li> <li>Early stages focus on interface correctness and learning signal, not end\u2011quality.</li> <li>Checkpoints are designed to be light (connector\u2011only / trainable\u2011rows\u2011only) until later stages.</li> </ul>"}]}